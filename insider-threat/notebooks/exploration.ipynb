{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Insider Threat Detection - Exploration Notebook\n",
        "\n",
        "This notebook provides a guided walkthrough of the insider threat detection project.\n",
        "It's designed for beginners to understand each step of the process.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. How to load and explore the dataset\n",
        "2. How schema detection works\n",
        "3. Feature engineering step-by-step\n",
        "4. Model predictions and explanations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add scripts to path\n",
        "sys.path.insert(0, '../scripts')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (use sample if full dataset not available)\n",
        "data_path = Path('../data/cert_dataset.csv')\n",
        "if not data_path.exists():\n",
        "    data_path = Path('../data/sample_cert_small.csv')\n",
        "    print(f\"Using sample dataset: {data_path}\")\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to Look For\n",
        "\n",
        "- **User column**: Identifies different users\n",
        "- **Date/Timestamp**: When events occurred\n",
        "- **IP addresses**: Source and destination\n",
        "- **File paths**: What files were accessed\n",
        "- **Success**: Whether actions succeeded\n",
        "- **Label**: Whether event is anomalous (if available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Unique users: {df['user'].nunique()}\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "if 'label' in df.columns:\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    print(f\"Anomaly rate: {df['label'].mean()*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Schema Detection\n",
        "\n",
        "Let's run the schema detection to understand our data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run schema detection (inline version)\n",
        "from schema_and_inventory import generate_schema\n",
        "\n",
        "schema = generate_schema(df, '../data/detected_schema.json')\n",
        "\n",
        "print(\"Schema Summary:\")\n",
        "for col_name, col_info in schema['columns'].items():\n",
        "    print(f\"\\n{col_name}:\")\n",
        "    print(f\"  Type: {col_info['type']}\")\n",
        "    print(f\"  Missing: {col_info['missing_count']} ({col_info['missing_percentage']:.1f}%)\")\n",
        "    print(f\"  Unique values: {col_info['unique_count']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Feature Engineering\n",
        "\n",
        "Now let's see how raw events are aggregated into features. We'll use the data_prep script to generate features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load or generate features\n",
        "features_path = Path('../data/features.csv')\n",
        "if features_path.exists():\n",
        "    features_df = pd.read_csv(features_path)\n",
        "    print(f\"âœ“ Loaded features from: {features_path}\")\n",
        "    print(f\"Features shape: {features_df.shape}\")\n",
        "    print(f\"\\nFeature columns: {features_df.columns.tolist()}\")\n",
        "    features_df.head()\n",
        "else:\n",
        "    print(\"âš  Features not found. Run this command in terminal:\")\n",
        "    print(\"   python scripts/data_prep.py --input data/cert_dataset.csv --output data/features.csv --split\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Distributions\n",
        "\n",
        "Let's visualize how features are distributed to understand the data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'features_df' in locals():\n",
        "    # Plot distributions\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    if 'total_events' in features_df.columns:\n",
        "        features_df['total_events'].hist(ax=axes[0, 0], bins=30, edgecolor='black')\n",
        "        axes[0, 0].set_title('Total Events Distribution', fontsize=12, fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('Total Events')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    if 'unique_src_ip' in features_df.columns:\n",
        "        features_df['unique_src_ip'].hist(ax=axes[0, 1], bins=20, edgecolor='black')\n",
        "        axes[0, 1].set_title('Unique Source IPs', fontsize=12, fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Unique IPs')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    if 'distinct_files' in features_df.columns:\n",
        "        features_df['distinct_files'].hist(ax=axes[1, 0], bins=30, edgecolor='black')\n",
        "        axes[1, 0].set_title('Distinct Files Accessed', fontsize=12, fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Files')\n",
        "        axes[1, 0].set_ylabel('Frequency')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    if 'avg_success' in features_df.columns:\n",
        "        features_df['avg_success'].hist(ax=axes[1, 1], bins=20, edgecolor='black')\n",
        "        axes[1, 1].set_title('Average Success Rate', fontsize=12, fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Success Rate')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nFeature Statistics:\")\n",
        "    print(features_df.describe())\n",
        "else:\n",
        "    print(\"âš  Load features first to see distributions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Model Predictions\n",
        "\n",
        "If you've trained models, let's load them and make predictions. This shows how the models work in practice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Load XGBoost model if available\n",
        "xgb_model_path = Path('../models/xgb_model.pkl')\n",
        "xgb_scaler_path = Path('../models/xgb_scaler.pkl')\n",
        "\n",
        "if xgb_model_path.exists() and xgb_scaler_path.exists():\n",
        "    model = joblib.load(xgb_model_path)\n",
        "    scaler = joblib.load(xgb_scaler_path)\n",
        "    \n",
        "    print(\"âœ“ Loaded XGBoost model and scaler\")\n",
        "    \n",
        "    # Make predictions on sample data\n",
        "    if 'features_df' in locals() and 'label' in features_df.columns:\n",
        "        feature_cols = [c for c in features_df.columns if c not in ['user', 'date', 'label']]\n",
        "        X = features_df[feature_cols].values\n",
        "        X_scaled = scaler.transform(X)\n",
        "        \n",
        "        predictions = model.predict(X_scaled)\n",
        "        probabilities = model.predict_proba(X_scaled)[:, 1]\n",
        "        \n",
        "        print(f\"\\nPredictions made: {len(predictions)}\")\n",
        "        print(f\"Anomalies predicted: {predictions.sum()}\")\n",
        "        \n",
        "        # Show some examples\n",
        "        results_df = features_df[['user', 'date']].copy()\n",
        "        results_df['prediction'] = predictions\n",
        "        results_df['probability'] = probabilities\n",
        "        if 'label' in features_df.columns:\n",
        "            results_df['actual'] = features_df['label']\n",
        "        \n",
        "        print(\"\\nTop 5 predicted anomalies:\")\n",
        "        display_cols = ['user', 'date', 'probability']\n",
        "        if 'actual' in results_df.columns:\n",
        "            display_cols.append('actual')\n",
        "        print(results_df.nlargest(5, 'probability')[display_cols])\n",
        "        \n",
        "        # Show confusion matrix if labels available\n",
        "        if 'actual' in results_df.columns:\n",
        "            from sklearn.metrics import confusion_matrix, classification_report\n",
        "            cm = confusion_matrix(results_df['actual'], results_df['prediction'])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(cm)\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(results_df['actual'], results_df['prediction']))\n",
        "    else:\n",
        "        print(\"âš  Features dataframe not loaded or labels missing\")\n",
        "else:\n",
        "    print(\"âš  Models not found. Train models first:\")\n",
        "    print(\"  python scripts/train_xgb.py --input data/features_train.csv --test_path data/features_test.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Isolation Forest model if available\n",
        "iso_model_path = Path('../models/iso_model.pkl')\n",
        "iso_scaler_path = Path('../models/iso_scaler.pkl')\n",
        "\n",
        "if iso_model_path.exists() and iso_scaler_path.exists():\n",
        "    iso_model = joblib.load(iso_model_path)\n",
        "    iso_scaler = joblib.load(iso_scaler_path)\n",
        "    \n",
        "    print(\"âœ“ Loaded Isolation Forest model\")\n",
        "    \n",
        "    if 'features_df' in locals():\n",
        "        feature_cols = [c for c in features_df.columns if c not in ['user', 'date', 'label']]\n",
        "        X = features_df[feature_cols].values\n",
        "        X_scaled = iso_scaler.transform(X)\n",
        "        \n",
        "        # Get anomaly scores (lower = more anomalous)\n",
        "        scores = iso_model.score_samples(X_scaled)\n",
        "        predictions = iso_model.predict(X_scaled)  # -1 for anomaly, 1 for normal\n",
        "        \n",
        "        iso_results = features_df[['user', 'date']].copy()\n",
        "        iso_results['anomaly_score'] = scores\n",
        "        iso_results['prediction'] = predictions\n",
        "        iso_results['is_anomaly'] = (predictions == -1)\n",
        "        \n",
        "        print(f\"\\nAnomalies detected: {iso_results['is_anomaly'].sum()}\")\n",
        "        print(f\"\\nTop 5 anomalies (lowest scores):\")\n",
        "        print(iso_results.nsmallest(5, 'anomaly_score')[['user', 'date', 'anomaly_score']])\n",
        "        \n",
        "        # Plot anomaly score distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(scores, bins=50, edgecolor='black', alpha=0.7)\n",
        "        plt.axvline(np.percentile(scores, 1), color='red', linestyle='--', \n",
        "                   label='Top 1% threshold')\n",
        "        plt.xlabel('Anomaly Score (lower = more anomalous)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Isolation Forest Anomaly Score Distribution')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"âš  Features dataframe not loaded\")\n",
        "else:\n",
        "    print(\"âš  Isolation Forest model not found. Train it first:\")\n",
        "    print(\"  python scripts/train_iso.py --input data/features_train.csv --contamination 0.01\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Feature Correlations\n",
        "\n",
        "Understanding which features are related can help interpret model behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'features_df' in locals():\n",
        "    # Select numeric features\n",
        "    numeric_features = features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'label' in numeric_features:\n",
        "        numeric_features.remove('label')\n",
        "    \n",
        "    if len(numeric_features) > 1:\n",
        "        # Create correlation matrix\n",
        "        corr_matrix = features_df[numeric_features].corr()\n",
        "        \n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                   center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "        plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\\nStrongest correlations:\")\n",
        "        # Get upper triangle (avoid duplicates)\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "        corr_pairs = corr_matrix.where(mask).stack().sort_values(ascending=False)\n",
        "        print(corr_pairs.head(5))\n",
        "    else:\n",
        "        print(\"âš  Not enough numeric features for correlation analysis\")\n",
        "else:\n",
        "    print(\"âš  Load features first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Time-Based Analysis\n",
        "\n",
        "Analyze user behavior patterns over time to identify trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'features_df' in locals() and 'date' in features_df.columns:\n",
        "    # Convert date to datetime\n",
        "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
        "    \n",
        "    # Plot activity over time\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # Total events over time\n",
        "    daily_events = features_df.groupby('date')['total_events'].sum()\n",
        "    axes[0].plot(daily_events.index, daily_events.values, marker='o', linewidth=2)\n",
        "    axes[0].set_title('Total Events Over Time', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xlabel('Date')\n",
        "    axes[0].set_ylabel('Total Events')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Average success rate over time\n",
        "    if 'avg_success' in features_df.columns:\n",
        "        daily_success = features_df.groupby('date')['avg_success'].mean()\n",
        "        axes[1].plot(daily_success.index, daily_success.values, marker='o', \n",
        "                    color='green', linewidth=2)\n",
        "        axes[1].set_title('Average Success Rate Over Time', fontsize=12, fontweight='bold')\n",
        "        axes[1].set_xlabel('Date')\n",
        "        axes[1].set_ylabel('Success Rate')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTime-based Statistics:\")\n",
        "    print(f\"Date range: {features_df['date'].min()} to {features_df['date'].max()}\")\n",
        "    print(f\"Days covered: {(features_df['date'].max() - features_df['date'].min()).days}\")\n",
        "else:\n",
        "    print(\"âš  Date column not found or features not loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "### What to Explore Further\n",
        "\n",
        "1. **Feature Engineering**: Try adding new features (weekend detection, file type analysis)\n",
        "2. **Model Comparison**: Compare XGBoost vs Isolation Forest performance\n",
        "3. **SHAP Explanations**: Generate detailed explanations for specific predictions\n",
        "4. **Hyperparameter Tuning**: Experiment with different model parameters\n",
        "5. **Visualization**: Create dashboards for monitoring anomalies\n",
        "\n",
        "### Useful Commands\n",
        "\n",
        "```bash\n",
        "# Generate SHAP explanations\n",
        "python scripts/explain_xgb_shap.py --model_path models/xgb_model.pkl --test_data data/features_test.csv\n",
        "\n",
        "# Run evaluation\n",
        "python scripts/evaluate.py --test_data data/features_test.csv\n",
        "\n",
        "# Start API server\n",
        "bash scripts/run_api.sh\n",
        "```\n",
        "\n",
        "### Documentation\n",
        "\n",
        "- **Tutorial**: `docs/tutorial_for_beginners.md` - Step-by-step guide\n",
        "- **Experiments**: `docs/experiments.md` - Hyperparameter tuning guide\n",
        "- **Tasks**: `TASKS.md` - Follow-up improvements\n",
        "- **Quick Start**: `QUICK_START.md` - Exact commands to run\n",
        "\n",
        "### Tips for Beginners\n",
        "\n",
        "- Start with the sample dataset to understand the workflow\n",
        "- Experiment with different contamination rates for Isolation Forest\n",
        "- Visualize feature distributions to understand data better\n",
        "- Check model performance metrics before deploying\n",
        "- Use SHAP to understand why users are flagged\n",
        "\n",
        "Happy exploring! ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
